name: Refresh CPLD + Build

on:
  workflow_dispatch:
  schedule:
    - cron: "17 6 * * 1-5"   # Weekdays 06:17 UTC

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Show repo tree (before)
        run: |
          pwd
          ls -la
          echo "---- scraper/ ----"
          ls -la scraper || true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pyyaml beautifulsoup4 lxml

      - name: Print models.yaml
        run: |
          echo "----- scraper/models.yaml -----"
          cat scraper/models.yaml

      # ---------- Diagnostics #1: probe Dell JSON endpoint ----------
      - name: Probe Dell JSON endpoint (R640)
        run: |
          python - <<'PY'
          import requests, time, json
          url = "https://www.dell.com/support/driver/en-us/ips/api/driverlist/fetchdriversbyproduct"
          params = {
              "productcode": "poweredge-r640",
              "oscode": "NAA",
              "lob": "POWEREDGE",
              "initialload": "true",
              "_": str(int(time.time()*1000)),
          }
          h = {"X-Requested-With":"XMLHttpRequest","User-Agent":"Mozilla/5.0 (ActionsProbe)"}
          try:
              r = requests.get(url, params=params, headers=h, timeout=45)
              print("JSON status:", r.status_code)
              print("JSON length:", len(r.text))
              data = r.json()
              items = data.get("DriverListData") or []
              cpld_items = [d for d in items if "CPLD" in (" ".join([str(d.get(k,"")) for k in ("DriverName","Category","ComponentType")])).upper()]
              print("JSON CPLD items:", len(cpld_items))
              print("First CPLD names:", [ (cpld_items[i].get("DriverName")) for i in range(min(3, len(cpld_items))) ])
          except Exception as e:
              print("JSON probe error:", e)
          PY

      # ---------- Diagnostics #2: probe Enterprise Catalog ----------
      - name: Probe Enterprise Catalog (Catalog.gz)
        run: |
          python - <<'PY'
          import requests, gzip, io, re
          CATALOG_URL = "https://downloads.dell.com/catalog/Catalog.gz"
          r = requests.get(CATALOG_URL, timeout=60)
          print("CAT status:", r.status_code)
          print("CAT bytes:", len(r.content))
          raw = gzip.GzipFile(fileobj=io.BytesIO(r.content)).read()
          text = None
          for enc in ("utf-16le","utf-16","utf-8-sig","utf-8"):
              try:
                  text = raw.decode(enc)
                  print("CAT decoded as:", enc)
                  break
              except Exception:
                  pass
          if text is None:
              text = raw.decode("utf-8", errors="replace")
              print("CAT decoded as: utf-8 (replace)")
          # Rough CPLD count
          cpld_count = len(re.findall(r"CPLD", text, flags=re.IGNORECASE))
          print("CAT CPLD token count:", cpld_count)
          # Show a small sample if present
          import re as _re
          m = _re.search(r"CPLD_Firmware_([0-9A-Z]{5})_", text)
          print("Sample CPLD driverid token:", m.group(1) if m else "NONE")
          PY

      - name: Resolve latest CPLD URLs (creates scraper/cpld_pages.auto.yaml)
        run: python scraper/resolve_cpld_urls.py || true

      - name: Show resolver output (overlay + report)
        run: |
          echo "----- cpld_pages.auto.yaml -----"
          test -f scraper/cpld_pages.auto.yaml && cat scraper/cpld_pages.auto.yaml || echo "MISSING"
          echo "----- cpld_pages.report.json -----"
          test -f scraper/cpld_pages.report.json && cat scraper/cpld_pages.report.json || echo "MISSING"

      - name: Fail if overlay is empty or malformed
        run: |
          python - <<'PY'
          import sys, yaml, pathlib
          p = pathlib.Path("scraper/cpld_pages.auto.yaml")
          if not p.exists():
              sys.exit("ERROR: Overlay missing (scraper/cpld_pages.auto.yaml)")
          data = yaml.safe_load(p.read_text(encoding="utf-8")) or {}
          if not isinstance(data, dict) or "CPLD_PAGES" not in data or not isinstance(data["CPLD_PAGES"], dict) or not data["CPLD_PAGES"]:
              print("DEBUG overlay:", data)
              sys.exit("ERROR: Overlay has no CPLD_PAGES or is empty. Check models.yaml productcode slugs & resolver logs.")
          print("Overlay looks good. Models resolved:", ", ".join(sorted(data["CPLD_PAGES"].keys())))
          PY

      - name: Commit overlay (if changed)
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add scraper/cpld_pages.auto.yaml scraper/cpld_pages.report.json
          git diff --cached --quiet || git commit -m "chore: refresh CPLD URLs"
          git diff --cached --quiet || git push

      - name: Run main scraper
        run: python scraper/scraper.py

      - name: Commit site output (if changed)
        run: |
          git add site/latest.json
          git diff --cached --quiet || git commit -m "chore: update latest.json"
          git diff --cached --quiet || git push
``
