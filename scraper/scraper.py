#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import re
import time
import yaml
from pathlib import Path

import requests
from bs4 import BeautifulSoup

ROOT = Path(__file__).resolve().parents[1]

# Load models (strings or dicts with {"name": ..., "productcode": ...})
MODELS = yaml.safe_load((ROOT / "scraper" / "models.yaml").read_text(encoding="utf-8"))["models"]

# --- REQUIRED overlay generated by:  python scraper/resolve_cpld_urls.py ---
auto_map_path = ROOT / "scraper" / "cpld_pages.auto.yaml"
if not auto_map_path.exists():
    raise SystemExit(
        "Missing scraper/cpld_pages.auto.yaml. "
        "Run:  python scraper/resolve_cpld_urls.py  first."
    )

_auto = yaml.safe_load(auto_map_path.read_text(encoding="utf-8")) or {}
if not isinstance(_auto, dict) or "CPLD_PAGES" not in _auto:
    raise SystemExit("scraper/cpld_pages.auto.yaml is malformed (missing 'CPLD_PAGES').")

CPLD_PAGES = _auto["CPLD_PAGES"]
if not isinstance(CPLD_PAGES, dict) or not CPLD_PAGES:
    raise SystemExit("No CPLD URLs were resolved for the specified models.")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; DellCPLDChecker/1.0; +https://github.com/<you>/dell-cpld-check)"
}

# ---- HTML parsing fallback (prefer resolver data when available) ----
VERSION_RE_TEXT = re.compile(r"Version\s+([0-9]+(?:\.[0-9]+){1,2})", re.IGNORECASE)
VERSION_RE_JSON = re.compile(r'releaseVersion"\s*:\s*"([0-9]+(?:\.[0-9]+){1,2})', re.IGNORECASE)

def parse_version_from_html(html: str) -> str | None:
    """Best-effort: try text extraction first, then look for preloaded JSON."""
    soup = BeautifulSoup(html, "lxml")
    text = soup.get_text(separator="\n", strip=True)

    m = VERSION_RE_TEXT.search(text)
    if m:
        return m.group(1)

    m2 = VERSION_RE_JSON.search(html)
    return m2.group(1) if m2 else None


def http_get(url: str, headers: dict, timeout: int = 30, tries: int = 2, backoff: float = 0.7):
    """Simple retry wrapper."""
    last_exc = None
    for i in range(tries):
        try:
            r = requests.get(url, headers=headers, timeout=timeout)
            r.raise_for_status()
            return r
        except Exception as e:
            last_exc = e
            if i < tries - 1:
                time.sleep(backoff * (i + 1))
    raise last_exc


def get_latest_for_model(model_name: str) -> dict:
    """
    Uses the dynamically generated DriversDetails URL for this model
    and returns { model, cpld (version), source (url) }.
    """
    url = CPLD_PAGES.get(model_name)
    if not url:
        return {"model": model_name, "error": "no_cpld_url"}

    # If your resolver also saved version metadata in a sidecar file,
    # prefer reading that here instead of fetching HTML.
    # For now we keep HTML fetch as a fallback:
    r = http_get(url, headers=HEADERS, timeout=30, tries=2, backoff=0.8)
    ver = parse_version_from_html(r.text)
    if not ver:
        return {"model": model_name, "error": "version_not_found_in_page", "source": url}

    return {"model": model_name, "cpld": ver, "source": url}


def main():
    out = {
        "generated_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        "models": []
    }

    for m in MODELS:
        model_name = m["name"] if isinstance(m, dict) else str(m)
        try:
            info = get_latest_for_model(model_name)
            info["model"] = model_name  # normalize to string
        except Exception as e:
            info = {"model": model_name, "error": str(e)}
        out["models"].append(info)

    site = ROOT / "site"
    site.mkdir(parents=True, exist_ok=True)

    (site / "latest.json").write_text(
        json.dumps(out, indent=2),
        encoding="utf-8"
    )


if __name__ == "__main__":
    main()
